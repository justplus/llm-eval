# 大模型评测平台

本项目旨在提供一个大模型评测平台，用户可以制定自己的测试集，使用该测试集对大模型能力进行测评，以选择最佳的大模型。

## 第一期迭代功能

- **模型管理**:
    - 支持 OpenAI 兼容模型 (系统内置) 和用户自建模型。
    - 用户可对自建模型进行增删改和可用性验证。
    - 模型列表页面展示所有模型，并提供"开始对话"入口。
- **对话页面**:
    - 支持单模型对话和多模型（最多3个）对比。
    - 可为每个模型配置独立的 System Prompt 和 Temperature。
    - 标准 Chatbox 对话界面。
    - 对话历史记录、加载和删除。
    - **新增功能**：多模型同时对话，支持实时并行流式响应。
- **用户登录**:
    - 账号密码登录。
    - 用户可修改密码。
    - 用户数据隔离。
    - **主题切换**：支持动态切换DaisyUI的30种内置主题，包括Light、Dark、Cupcake、Synthwave等，主题选择会自动保存到本地存储。下拉菜单支持点击外部区域自动收起、ESC键关闭、焦点管理等交互优化。
- **数据集管理**:
    - 支持系统和自建数据集管理。
    - 数据集预览和筛选。
    - 按照子数据集和用途查看数据内容。
    - 使用DaisyUI文件输入组件进行数据集文件上传，提供更好的用户体验。
    - 自建数据集创建时自动设置发布日期为当前日期。
    - 数据集预览支持动态字段显示，根据实际数据结构自动生成表头和列。
    - 字段顺序保持：通过服务器端返回字段顺序信息，确保表格列顺序在不同页面和刷新后保持一致。
    - **数据集权限控制**：实现严格的数据集访问权限，用户只能看到：
      - 自己创建的所有数据集（无论是否公开）
      - 别人创建的公开数据集
      - 系统数据集
      - 确保私有数据集的安全性，防止未授权访问
- **模型评估**:
    - 基于测试数据集评估模型性能。
    - 支持选择一个或多个数据集对模型进行评估。
    - **数据集选择范围**：可以选择自己创建的所有数据集、别人公开的数据集和系统数据集。
    - 使用裁判模型对模型回答进行评分。
    - 评估结果实时更新和存储。
    - 查看评估历史和详细评估报告。

- **模型性能评估**:
    - 支持选择模型、数据集、并发路数和请求数量发起性能评估。
    - **数据集选择范围**：支持自己创建的自建数据集和别人公开的自建数据集，确保数据安全和可控性。
    - 使用数据集的实际文件路径进行评估，而不是数据集名称。
    - 异步执行性能基准测试 (基于 `evalscope.perf.main`)。
    - 结果包括汇总指标 (如吞吐量、延迟) 和百分位指标。
    - 美观的表格展示性能指标，支持分类显示和颜色编码。
    - **交互式指标说明**：指标名称带有虚下划线，点击可弹出详细说明，包括指标定义、计算公式等信息。
    - 改进的用户界面，包括图标、统计信息和响应式设计。
    - **界面优化**：表格操作按钮垂直居中对齐，删除确认使用DaisyUI风格的模态框。
    - 存储和展示历史性能评估任务及详细结果。
    - 提供仪表盘入口和导航菜单入口。

## 多模型对话功能

平台支持与多个模型同时对话，主要特点包括：

- **同时选择多个模型**：用户可以在侧边栏配置区域添加最多3个不同的模型。
- **独立模型配置**：每个模型可以设置不同的System Prompt和Temperature参数。
- **并行响应处理**：向所有选定的模型并行发送相同的用户消息。
- **实时流式显示**：每个模型的响应都实时流式显示在界面上，互不干扰。
- **响应对比**：同时看到多个模型对同一问题的不同回答，方便比较和评估。
- **错误隔离**：单个模型的错误不会影响其他模型的响应。
- **响应持久化**：所有模型的响应都会被记录到数据库中，供后续查看和分析。

### 使用方法

1. 在对话页面右侧的"模型配置"面板中，可以添加多个模型（使用"添加模型"按钮）。
2. 为每个模型选择具体的AI模型类型并根据需要调整参数。
3. 在消息输入框中输入您的问题，点击"发送"。
4. 所有选定的模型将同时接收到您的问题并开始生成回答。
5. 每个模型的回答会实时显示在对话界面中，并标有相应的模型名称标签。

## 模型评估功能

模型评估功能允许用户对模型在多个数据集上的表现进行全面评估，主要特点包括：

- **选择自定义模型**：选择任意已配置的模型进行评估。
- **选择数据集**：选择一个或多个已启用的数据集（系统数据集或自建数据集）。
- **支持自建数据集评估**：可以评估用户上传的自定义数据集，支持选择题(MCQ)和问答题(QA)格式。
- **配置生成参数**：设置温度和最大token等参数来控制模型响应的生成。
- **选择裁判模型**：选择一个模型（系统或自定义）作为裁判，对主模型的回答进行评分。
- **异步评估处理**：评估任务在后台异步执行，不影响其他操作。
- **实时状态更新**：评估页面会自动刷新，显示当前评估状态。
- **评估结果存储**：所有评估结果被保存，可随时查看。
- **智能问题格式解析**：自动识别并解析Python字典或JSON格式的问题结构，以简洁的对话形式展示历史对话和当前问题，使用图标区分用户和助手角色，支持长文本自动换行。
- **模型回答格式优化**：自动清理模型回答中的JSON代码块标记（如```json和```），并将JSON格式压缩为紧凑显示，提高可读性。
  - 简单数值评分：直接的数值分数
  - 复合评分：支持意图识别和槽位填充的联合评估，自动计算 `intent_result * slot_f1` 作为最终分数
    - 复合评分格式示例：`{"intent_result": true, "slots_result": {"miss_count": 1, "correct_count": 1, "fail_count": 0}}`
    - 系统会自动计算槽位的F1分数，然后与意图识别结果相乘得到最终分数
- **高级筛选功能**：
  - 按问题内容进行文本搜索
  - 按分数范围进行筛选（0.0-1.0），通过数字输入框设置最低和最高分数
  - 筛选条件在分页时自动保持

### 使用方法

1. 在仪表盘中点击"创建评估"按钮，或直接进入"评估历史"页面点击"创建新评估"。
2. 在创建评估页面中，选择要评估的模型和裁判模型。
3. 设置温度和最大token数等生成参数。
4. 选择一个或多个要用于评估的数据集，并指定子数据集和分割（如适用）。
5. 点击"开始评估"按钮启动评估流程。
6. 系统将异步执行评估任务，并在完成后显示评估结果。
7. 在"评估历史"页面可以查看所有历史评估记录和详细结果。

## 技术栈

-   **前端**: Flask服务端渲染, DaisyUI, Tailwind CSS
-   **后端**: Python Flask
-   **数据库**: MySQL

## 开发环境配置

### 🔄 自动重载功能

项目支持代码修改后自动重载，提供高效的开发体验：

#### 启动方式

1. **使用开发脚本（推荐）**：
   ```bash
   python dev.py
   ```

2. **使用标准启动脚本**：
   ```bash
   python run.py
   ```

3. **使用Flask命令**：
   ```bash
   flask run
   ```

#### 开发脚本选项

```bash
# 基本启动
python dev.py

# 指定端口
python dev.py --port 8000

# 启用SQL查询日志
python dev.py --sql-echo

# 禁用自动重载
python dev.py --no-reload

# 查看所有选项
python dev.py --help
```

#### 自动重载特性

- **Python代码修改**：自动重启应用服务器
- **模板文件修改**：自动重载模板，无需重启
- **静态文件修改**：开发环境下缓存时间短，便于调试
- **配置文件修改**：自动应用新配置

#### 环境配置

项目支持多环境配置：

- **开发环境** (`development`)：
  - 启用调试模式和自动重载
  - 模板自动重载
  - 详细的错误信息
  - 静态文件缓存时间短

- **生产环境** (`production`)：
  - 禁用调试模式
  - 优化性能配置
  - 静态文件长期缓存

通过设置环境变量 `FLASK_ENV` 来切换环境：
```bash
export FLASK_ENV=development  # 开发环境
export FLASK_ENV=production   # 生产环境
```

## 目录结构

```
project_root/
├── app/ # Flask应用主目录
│   ├── models.py # 数据库模型
│   ├── routes/ # 路由和视图函数
│   │   ├── auth.py # 认证相关路由
│   │   ├── chat_routes.py # 聊天相关路由
│   │   ├── main.py # 主页和基础路由
│   │   ├── models_routes.py # 模型管理路由
│   │   ├── dataset_routes.py # 数据集相关路由
│   │   ├── evaluation_routes.py # 模型评估路由
│   │   └── perf_eval.py # 模型性能评估路由
│   ├── services/ # 业务逻辑服务
│   │   ├── chat_service.py # 聊天相关服务
│   │   ├── model_service.py # 模型管理服务
│   │   ├── dataset_service.py # 数据集相关服务
│   │   ├── user_service.py # 用户相关服务
│   │   └── evaluation_service.py # 模型评估服务
│   ├── static/ # 静态资源
│   │   ├── css/ # 样式表
│   │   ├── js/ # JavaScript文件
│   │   └── images/ # 图片资源
│   ├── templates/ # HTML模板
│   │   ├── auth/ # 认证相关模板
│   │   ├── chat/ # 聊天界面模板
│   │   ├── models/ # 模型管理模板
│   │   ├── datasets/ # 数据集管理模板
│   │   ├── evaluations/ # 模型评估模板
│   │   └── base.html # 基础模板
│   ├── forms.py # 表单定义
│   ├── config.py # 配置文件
│   └── __init__.py # Flask应用初始化
├── run.py # 应用入口点
├── migrations/ # 数据库迁移文件
└── README.md
```

## 安全性

- 本项目已移除 CSRF 保护，因为其部署在内部网络环境。