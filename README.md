# 大模型评测平台

本项目旨在提供一个大模型评测平台，用户可以制定自己的测试集，使用该测试集对大模型能力进行测评，以选择最佳的大模型。
- **模型管理**:
    - 支持 OpenAI 兼容模型 (系统内置) 和用户自建模型。
    - 用户可对自建模型进行增删改和可用性验证。
    - 模型列表页面展示所有模型，并提供"开始对话"入口。
- **对话页面**:
    - 支持单模型对话和多模型（最多3个）对比。
    - 可为每个模型配置独立的 System Prompt 和 Temperature。
    - 标准 Chatbox 对话界面。
    - 对话历史记录、加载和删除。
    - 多模型同时对话，支持实时并行流式响应。
- **用户登录**:
    - 账号密码登录。
    - 用户可修改密码。
    - 用户数据隔离。
- **数据集管理**:
    - 支持系统和自建数据集管理。
    - 数据集预览和筛选。
    - 按照子数据集和用途查看数据内容。
    - 使用DaisyUI文件输入组件进行数据集文件上传，提供更好的用户体验。
    - 自建数据集创建时自动设置发布日期为当前日期。
    - 数据集预览支持动态字段显示，根据实际数据结构自动生成表头和列。
    - 字段顺序保持：通过服务器端返回字段顺序信息，确保表格列顺序在不同页面和刷新后保持一致。
    - **数据集权限控制**：实现严格的数据集访问权限，用户只能看到：
      - 自己创建的所有数据集（无论是否公开）
      - 别人创建的公开数据集
      - 系统数据集
      - 确保私有数据集的安全性，防止未授权访问
- **模型评估**:
    - 基于测试数据集评估模型性能。
    - 支持选择一个或多个数据集对模型进行评估。
    - **数据集选择范围**：可以选择自己创建的所有数据集、别人公开的数据集和系统数据集。
    - 使用裁判模型对模型回答进行评分。
    - 评估结果实时更新和存储。
    - 查看评估历史和详细评估报告。

- **模型性能评估**:
    - 支持选择模型、数据集、并发路数和请求数量发起性能评估。
    - **数据集选择范围**：支持自己创建的自建数据集和别人公开的自建数据集，确保数据安全和可控性。
    - 使用数据集的实际文件路径进行评估，而不是数据集名称。
    - 异步执行性能基准测试 (基于 `evalscope.perf.main`)。
    - 结果包括汇总指标 (如吞吐量、延迟) 和百分位指标。
    - 美观的表格展示性能指标，支持分类显示和颜色编码。
    - **交互式指标说明**：指标名称带有虚下划线，点击可弹出详细说明，包括指标定义、计算公式等信息。
    - 改进的用户界面，包括图标、统计信息和响应式设计。
    - **界面优化**：表格操作按钮垂直居中对齐，删除确认使用DaisyUI风格的模态框。
    - 存储和展示历史性能评估任务及详细结果。
    - 提供仪表盘入口和导航菜单入口。

## 技术栈
-   **前端**: Flask服务端渲染, DaisyUI, Tailwind CSS
-   **后端**: Python Flask
-   **数据库**: MySQL

## 开发环境配置

### 🔄 自动重载功能

项目支持代码修改后自动重载，提供高效的开发体验：

#### 启动方式

1. **使用开发脚本（推荐）**：
   ```bash
   python dev.py
   ```

2. **使用标准启动脚本**：
   ```bash
   python run.py
   ```

3. **使用Flask命令**：
   ```bash
   flask run
   ```

#### 开发脚本选项

```bash
# 基本启动
python dev.py

# 指定端口
python dev.py --port 8000

# 启用SQL查询日志
python dev.py --sql-echo

# 禁用自动重载
python dev.py --no-reload

# 查看所有选项
python dev.py --help
```

```bash
docker build -t llm-eva .
docker save -o llm-eva.tar llm-eva:latest
docker load -i llm-eva.tar
```

#### 环境配置

项目支持多环境配置：

- **开发环境** (`development`)：
  - 启用调试模式和自动重载
  - 模板自动重载
  - 详细的错误信息
  - 静态文件缓存时间短

- **生产环境** (`production`)：
  - 禁用调试模式
  - 优化性能配置
  - 静态文件长期缓存

通过设置环境变量 `FLASK_ENV` 来切换环境：
```bash
export FLASK_ENV=development  # 开发环境
export FLASK_ENV=production   # 生产环境
```

## 目录结构

```
project_root/
├── app/ # Flask应用主目录
│   ├── models.py # 数据库模型
│   ├── routes/ # 路由和视图函数
│   │   ├── auth.py # 认证相关路由
│   │   ├── chat_routes.py # 聊天相关路由
│   │   ├── main.py # 主页和基础路由
│   │   ├── models_routes.py # 模型管理路由
│   │   ├── dataset_routes.py # 数据集相关路由
│   │   ├── evaluation_routes.py # 模型评估路由
│   │   └── perf_eval.py # 模型性能评估路由
│   ├── services/ # 业务逻辑服务
│   │   ├── chat_service.py # 聊天相关服务
│   │   ├── model_service.py # 模型管理服务
│   │   ├── dataset_service.py # 数据集相关服务
│   │   ├── user_service.py # 用户相关服务
│   │   └── evaluation_service.py # 模型评估服务
│   ├── static/ # 静态资源
│   │   ├── css/ # 样式表
│   │   ├── js/ # JavaScript文件
│   │   └── images/ # 图片资源
│   ├── templates/ # HTML模板
│   │   ├── auth/ # 认证相关模板
│   │   ├── chat/ # 聊天界面模板
│   │   ├── models/ # 模型管理模板
│   │   ├── datasets/ # 数据集管理模板
│   │   ├── evaluations/ # 模型评估模板
│   │   └── base.html # 基础模板
│   ├── forms.py # 表单定义
│   ├── config.py # 配置文件
│   └── __init__.py # Flask应用初始化
├── run.py # 应用入口点
├── migrations/ # 数据库迁移文件
└── README.md
```

## 安全性

- 本项目已移除 CSRF 保护，因为其部署在内部网络环境。

### 使用方法

1. 选择支持推理的模型（如 o1、o1-mini）
2. 发送消息，如果模型返回推理内容，会自动显示思考过程
3. 点击"💭 思考过程"可以展开或收起思考内容

### 测试功能

发送包含"测试推理"的消息可以触发模拟推理内容的显示，用于测试功能是否正常工作。

### 技术实现

- **后端检测**：检测 `response.choices[0].message.reasoning` 字段
- **流式处理**：支持推理内容的实时流式传输和显示
- **存储格式**：使用标准化的 `<think></think><answer></answer>` 格式
- **前端解析**：JavaScript 自动解析并创建可折叠的思考过程界面
- **样式设计**：使用渐变背景和等宽字体显示思考过程，保证良好的可读性


Highlight: 
# 公有云、私有云、本地部署的模型一键PK，用于快速验证新模型效果 
# 内置了用于模型测评的通用数据集，提供了数据预览和下载的功能
# 支持自建数据集管理，支持数据集团队内共享，再也不用为数据集不同意效果不一致而发愁
# 支持对选择题、主观题(裁判打分)、分类问题、抽槽问题等常用的数据集进行效果度量，支持自定义测评指标
# 支持对模型效率进行一键度量
# 提供了所有历史数据的管理，方便对微调/量化后模型进行对比

Todo:
# 模型安全性测评(processing)
# 支持VL模型(planning)
# 支持Ragas测评(processing)
# 支持对思考模型的思考不足/过度思考能力进行测评(planning)
# 支持对智能体进行测评(planning)