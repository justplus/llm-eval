# 大模型评测平台

本项目旨在提供一个大模型评测平台，用户可以制定自己的测试集，使用该测试集对大模型能力进行测评，以选择最佳的大模型。

## 第一期迭代功能

- **模型管理**:
    - 支持 OpenAI 兼容模型 (系统内置) 和用户自建模型。
    - 用户可对自建模型进行增删改和可用性验证。
    - 模型列表页面展示所有模型，并提供"开始对话"入口。
- **对话页面**:
    - 支持单模型对话和多模型（最多3个）对比。
    - 可为每个模型配置独立的 System Prompt 和 Temperature。
    - 标准 Chatbox 对话界面。
    - 对话历史记录、加载和删除。
    - **新增功能**：多模型同时对话，支持实时并行流式响应。
- **用户登录**:
    - 账号密码登录。
    - 用户可修改密码。
    - 用户数据隔离。
- **数据集管理**:
    - 支持系统和自建数据集管理。
    - 数据集预览和筛选。
    - 按照子数据集和用途查看数据内容。
- **模型评估**:
    - 基于测试数据集评估模型性能。
    - 支持选择一个或多个数据集对模型进行评估。
    - 使用裁判模型对模型回答进行评分。
    - 评估结果实时更新和存储。
    - 查看评估历史和详细评估报告。

- **模型性能评估**:
    - 支持选择模型、数据集、并发路数和请求数量发起性能评估。
    - 异步执行性能基准测试 (基于 `evalscope.perf.main`)。
    - 结果包括汇总指标 (如吞吐量、延迟) 和百分位指标。
    - 存储和展示历史性能评估任务及详细结果。
    - 提供仪表盘入口和导航菜单入口。

## 多模型对话功能

平台支持与多个模型同时对话，主要特点包括：

- **同时选择多个模型**：用户可以在侧边栏配置区域添加最多3个不同的模型。
- **独立模型配置**：每个模型可以设置不同的System Prompt和Temperature参数。
- **并行响应处理**：向所有选定的模型并行发送相同的用户消息。
- **实时流式显示**：每个模型的响应都实时流式显示在界面上，互不干扰。
- **响应对比**：同时看到多个模型对同一问题的不同回答，方便比较和评估。
- **错误隔离**：单个模型的错误不会影响其他模型的响应。
- **响应持久化**：所有模型的响应都会被记录到数据库中，供后续查看和分析。

### 使用方法

1. 在对话页面右侧的"模型配置"面板中，可以添加多个模型（使用"添加模型"按钮）。
2. 为每个模型选择具体的AI模型类型并根据需要调整参数。
3. 在消息输入框中输入您的问题，点击"发送"。
4. 所有选定的模型将同时接收到您的问题并开始生成回答。
5. 每个模型的回答会实时显示在对话界面中，并标有相应的模型名称标签。

## 模型评估功能

模型评估功能允许用户对模型在多个数据集上的表现进行全面评估，主要特点包括：

- **选择自定义模型**：选择任意已配置的模型进行评估。
- **选择数据集**：选择一个或多个已启用的数据集（系统数据集或自建数据集）。
- **配置生成参数**：设置温度和最大token等参数来控制模型响应的生成。
- **选择裁判模型**：选择一个模型（系统或自定义）作为裁判，对主模型的回答进行评分。
- **异步评估处理**：评估任务在后台异步执行，不影响其他操作。
- **实时状态更新**：评估页面会自动刷新，显示当前评估状态。
- **评估结果存储**：所有评估结果被保存，可随时查看。
- **详细评估报告**：包含每个问题的模型回答、参考答案和裁判评价，以及总体评分。

### 使用方法

1. 在仪表盘中点击"创建评估"按钮，或直接进入"评估历史"页面点击"创建新评估"。
2. 在创建评估页面中，选择要评估的模型和裁判模型。
3. 设置温度和最大token数等生成参数。
4. 选择一个或多个要用于评估的数据集，并指定子数据集和分割（如适用）。
5. 点击"开始评估"按钮启动评估流程。
6. 系统将异步执行评估任务，并在完成后显示评估结果。
7. 在"评估历史"页面可以查看所有历史评估记录和详细结果。

## 技术栈

-   **前端**: Flask服务端渲染, DaisyUI, Tailwind CSS
-   **后端**: Python Flask
-   **数据库**: MySQL

## 目录结构

```
project_root/
├── app/ # Flask应用主目录
│   ├── models.py # 数据库模型
│   ├── routes/ # 路由和视图函数
│   │   ├── auth.py # 认证相关路由
│   │   ├── chat_routes.py # 聊天相关路由
│   │   ├── main.py # 主页和基础路由
│   │   ├── models_routes.py # 模型管理路由
│   │   ├── dataset_routes.py # 数据集相关路由
│   │   ├── evaluation_routes.py # 模型评估路由
│   │   └── perf_eval.py # 模型性能评估路由
│   ├── services/ # 业务逻辑服务
│   │   ├── chat_service.py # 聊天相关服务
│   │   ├── model_service.py # 模型管理服务
│   │   ├── dataset_service.py # 数据集相关服务
│   │   ├── user_service.py # 用户相关服务
│   │   └── evaluation_service.py # 模型评估服务
│   ├── static/ # 静态资源
│   │   ├── css/ # 样式表
│   │   ├── js/ # JavaScript文件
│   │   └── images/ # 图片资源
│   ├── templates/ # HTML模板
│   │   ├── auth/ # 认证相关模板
│   │   ├── chat/ # 聊天界面模板
│   │   ├── models/ # 模型管理模板
│   │   ├── datasets/ # 数据集管理模板
│   │   ├── evaluations/ # 模型评估模板
│   │   └── base.html # 基础模板
│   ├── forms.py # 表单定义
│   ├── config.py # 配置文件
│   └── __init__.py # Flask应用初始化
├── run.py # 应用入口点
├── migrations/ # 数据库迁移文件
└── README.md
```

## 安全性

- 本项目已移除 CSRF 保护，因为其部署在内部网络环境。

## 更新日志

- 2024-07-28: 修复了token过期后刷新页面提示403错误时，未自动跳转到登录页面的问题。
- 2024-07-31: 新增数据集格式字段(format)，支持选择题格式(MCQ)和问答题格式(QA)两种类型，并在数据集列表页面展示格式标签。
- 2024-08-01: 将数据集的sample_data字段修改为dataset_info字段，用于存储数据集的结构信息，包含子数据集、字段定义和数据分割等信息。
- 2024-08-05: 新增数据集预览功能，将下载数据按钮改为预览数据按钮，支持按照子数据集和用途筛选数据，并以分页形式展示数据内容。
- 2024-08-07: 新增模型评估功能，支持选择模型和数据集进行批量评估，使用裁判模型对结果评分，并提供详细评估报告和历史记录查看。
- [[当前日期]]: 新增模型性能评估模块，支持对模型进行吞吐量、延迟等性能指标的基准测试和结果展示。
